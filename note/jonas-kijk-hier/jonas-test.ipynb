{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic MERCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mercs\n",
    "import numpy as np\n",
    "from mercs.tests import load_iris, default_dataset\n",
    "from mercs.core import Mercs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testdrive\n",
    "\n",
    "Here a small MERCS testdrive for what I suppose you'll need. First, let us generate a basic dataset. Some utility-functions are integrated in MERCS so that goes like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.32215078, -0.79095656,  0.51626038, -0.48405183,  1.        ],\n",
       "       [-1.81333888,  0.62979917,  0.97189032,  1.93749521,  1.        ],\n",
       "       [-0.86602753, -0.7458731 ,  0.34843253, -1.04698596,  1.        ],\n",
       "       [-0.4310274 , -0.19616028, -2.8226608 , -1.44558936,  0.        ],\n",
       "       [-2.51221012, -3.31467898,  4.22965537,  0.14797735,  0.        ],\n",
       "       [-0.03665064,  0.07566481,  2.15518439,  1.21783179,  0.        ],\n",
       "       [-1.12285279, -1.20080724,  0.60774537, -0.17858784,  0.        ],\n",
       "       [-1.12416734, -1.02493785, -1.05914756, -1.90971485,  0.        ],\n",
       "       [-1.2152382 , -2.86791935,  1.10903289, -3.02430913,  1.        ],\n",
       "       [-0.3775988 , -0.7107917 , -0.73213335, -2.3530741 ,  0.        ],\n",
       "       [ 0.12210504,  0.44135258,  2.26863961, -0.08306471,  0.        ],\n",
       "       [ 0.5373043 , -0.2780344 ,  2.43787375,  2.68441233,  0.        ],\n",
       "       [ 1.47020219, -0.46290396, -2.32769304,  1.26565925,  1.        ],\n",
       "       [ 1.14584153, -0.09836175, -2.06298865,  2.47742251,  1.        ],\n",
       "       [-0.49319527,  2.34222823,  1.08518984,  1.8631808 ,  1.        ],\n",
       "       [-1.35083251,  0.08519989,  0.55188636, -0.17132777,  0.        ],\n",
       "       [-1.31578006, -2.4124212 ,  1.07133708, -2.31703991,  1.        ],\n",
       "       [-0.32742585, -0.20186463,  1.07377589, -0.28283357,  1.        ],\n",
       "       [-0.08899953,  0.37123321, -1.35779252,  1.26150277,  1.        ],\n",
       "       [-1.77134206, -1.96791996,  0.59301804, -0.77357532,  0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = default_dataset(n_features=4 * 10 ** 0)\n",
    "train[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a MERCS model. To know what options you have, come talk to me or dig in the code. For induction, `nb_targets` and `nb_iterations` matter most. Number of targets speaks for itself, number of iterations manages the amount of trees _for each target_. With `n_jobs` you can do multi-core learning (with joblib, really basic, but works fine on single machine), that makes stuff faster. `fraction_missing` sets the amount of attributes that is missing for a tree. However, this parameter only has an effect if you use the `random` selection algorithm. The alternative is the `base` algorithm, which selects targets, and uses all the rest as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Mercs(\n",
    "    max_depth=4,\n",
    "    selection_algorithm=\"random\",\n",
    "    fraction_missing=0.6,\n",
    "    nb_targets=2,\n",
    "    nb_iterations=2,\n",
    "    n_jobs=8,\n",
    "    verbose=1,\n",
    "    inference_algorithm=\"ndask\",\n",
    "    max_steps=8,\n",
    "    prediction_algorithm=\"vit\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to specify the nominal attributes yourself. This determines whether a regressor or a classifier is learned for that target. MERCS takes care of grouping targets such that no mixed sets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nominal_ids = {train.shape[1]-1}\n",
    "nominal_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cw/dtailocal/repos/mercs/src/mercs/algo/induction.py:100: UserWarning: \n",
      "        Training is being parallellized using Joblib. Number of jobs = 8\n",
      "        \n",
      "  warnings.warn(msg)\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train, nominal_attributes=nominal_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have learned trees with two targets, but only a single target was nominal. If MERCS worked well, it should have learned single-target classifiers (for attribute 4) and multi-target regressors for all other target sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=5191, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=5191, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=5191, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       presort=False, random_state=5191, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=6265, splitter='best'),\n",
       " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=6265, splitter='best')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.m_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that looks good already. Let's examine up close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1,  1,  0],\n",
       "       [ 1,  1,  0,  0, -1],\n",
       "       [-1,  1, -1,  1,  0],\n",
       "       [ 1, -1,  1,  0, -1],\n",
       "       [-1, -1,  0, -1,  1],\n",
       "       [ 0, -1, -1, -1,  1]], dtype=int8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.m_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the matrix that summarizes everything. This can be dense to parse, and there's alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Tree with id:          0\n",
      "    has source attributes: [4]\n",
      "    has target attributes: [2, 3],\n",
      "    and predicts numeric attributes\n",
      "    \n",
      "\n",
      "    Tree with id:          1\n",
      "    has source attributes: [2, 3]\n",
      "    has target attributes: [0, 1],\n",
      "    and predicts numeric attributes\n",
      "    \n",
      "\n",
      "    Tree with id:          2\n",
      "    has source attributes: [4]\n",
      "    has target attributes: [1, 3],\n",
      "    and predicts numeric attributes\n",
      "    \n",
      "\n",
      "    Tree with id:          3\n",
      "    has source attributes: [3]\n",
      "    has target attributes: [0, 2],\n",
      "    and predicts numeric attributes\n",
      "    \n",
      "\n",
      "    Tree with id:          4\n",
      "    has source attributes: [2]\n",
      "    has target attributes: [4],\n",
      "    and predicts nominal attributes\n",
      "    \n",
      "\n",
      "    Tree with id:          5\n",
      "    has source attributes: [0]\n",
      "    has target attributes: [4],\n",
      "    and predicts nominal attributes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for m_idx, m in enumerate(clf.m_list):\n",
    "    msg = \"\"\"\n",
    "    Tree with id:          {}\n",
    "    has source attributes: {}\n",
    "    has target attributes: {},\n",
    "    and predicts {} attributes\n",
    "    \"\"\".format(m_idx, m.desc_ids, m.targ_ids, m.out_kind)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mercs",
   "language": "python",
   "name": "mercs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
