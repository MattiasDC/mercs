{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 This is the documentation of the mercs package. Short Description \u00b6 MERCS Info about the documentation \u00b6 For full documentation visit mkdocs.org . Tests \u00b6 Equations go like this e^{i\\pi}-1=0 e^{i\\pi}-1=0 Code blocks go like this msg = \"Hello world\" print ( msg ) And you can add all kinds of cool blocks to draw attention to something, Example This is a perfect example of such an admonition block as they are called. They come in many types and their icons adapt automagically. Cf. the github source (click the edit button) to see how all of this came to be. Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome"},{"location":"#welcome","text":"This is the documentation of the mercs package.","title":"Welcome"},{"location":"#short-description","text":"MERCS","title":"Short Description"},{"location":"#info-about-the-documentation","text":"For full documentation visit mkdocs.org .","title":"Info about the documentation"},{"location":"#tests","text":"Equations go like this e^{i\\pi}-1=0 e^{i\\pi}-1=0 Code blocks go like this msg = \"Hello world\" print ( msg ) And you can add all kinds of cool blocks to draw attention to something, Example This is a perfect example of such an admonition block as they are called. They come in many types and their icons adapt automagically. Cf. the github source (click the edit button) to see how all of this came to be.","title":"Tests"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"quickstart/","text":"Quickstart \u00b6 Preliminaries \u00b6 Imports \u00b6 import mercs import numpy as np from mercs.tests import load_iris , default_dataset from mercs.core import Mercs import pandas as pd Fit \u00b6 Here a small MERCS testdrive for what I suppose you'll need. First, let us generate a basic dataset. Some utility-functions are integrated in MERCS so that goes like this train , test = default_dataset ( n_features = 8 * 10 ** 0 ) df = pd . DataFrame ( train ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 -1.322151 -0.790957 0.516260 -0.484052 1.0 1 -1.813339 0.629799 0.971890 1.937495 1.0 2 -0.866028 -0.745873 0.348433 -1.046986 1.0 3 -0.431027 -0.196160 -2.822661 -1.445589 0.0 4 -2.512210 -3.314679 4.229655 0.147977 0.0 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 count 800.000000 800.000000 800.000000 800.000000 800.000000 mean -0.557043 -0.559566 0.006219 0.009929 0.495000 std 1.320172 1.395167 1.513970 1.541863 0.500288 min -4.843622 -4.530188 -4.497019 -4.653281 0.000000 25% -1.349024 -1.500278 -1.026567 -1.076323 0.000000 50% -0.705760 -0.698342 0.344050 0.040782 0.000000 75% 0.237204 0.271365 1.151239 1.071138 1.000000 max 3.371588 3.789293 4.229655 5.090814 1.000000 Now let's train a MERCS model. To know what options you have, come talk to me or dig in the code. For induction, nb_targets and nb_iterations matter most. Number of targets speaks for itself, number of iterations manages the amount of trees for each target . With n_jobs you can do multi-core learning (with joblib, really basic, but works fine on single machine), that makes stuff faster. fraction_missing sets the amount of attributes that is missing for a tree. However, this parameter only has an effect if you use the random selection algorithm. The alternative is the base algorithm, which selects targets, and uses all the rest as input. clf = Mercs ( max_depth = 4 , selection_algorithm = \"random\" , fraction_missing = 0.6 , nb_targets = 2 , nb_iterations = 2 , n_jobs = 8 , verbose = 1 , inference_algorithm = \"dask\" , max_steps = 8 , prediction_algorithm = \"it\" , ) You have to specify the nominal attributes yourself. This determines whether a regressor or a classifier is learned for that target. MERCS takes care of grouping targets such that no mixed sets are created. nominal_ids = { train . shape [ 1 ] - 1 } nominal_ids {4} clf . fit ( train , nominal_attributes = nominal_ids ) /cw/dtaijupiter/NoCsBack/dtai/elia/mercs/src/mercs/algo/induction.py:191: UserWarning: Training is being parallellized using Joblib. Number of jobs = 8 warnings.warn(msg) [Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=8)]: Done 6 out of 6 | elapsed: 2.3s finished /cw/dtaijupiter/NoCsBack/dtai/elia/miniconda/envs/rwrf/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) So, now we have learned trees with two targets, but only a single target was nominal. If MERCS worked well, it should have learned single-target classifiers (for attribute 4) and multi-target regressors for all other target sets. clf . m_list [<mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf972151d0>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf97220650>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf97220710>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf9729cc90>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf97204d10>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcfd0465b50>] So, that looks good already. Let's examine up close. clf . m_codes array([[-1, 0, 0, -1, 1], [-1, -1, 0, 0, 1], [ 1, 0, 1, 0, -1], [-1, 1, -1, 1, 0], [ 1, 0, 1, 0, 0], [-1, 1, 0, 1, -1]]) That's the matrix that summarizes everything. This can be dense to parse, and there's alternatives to gain insights, for instance; for m_idx , m in enumerate ( clf . m_list ): msg = \"\"\" Tree with id: {} has source attributes: {} has target attributes: {} , and predicts {} attributes \"\"\" . format ( m_idx , m . desc_ids , m . targ_ids , m . out_kind ) print ( msg ) Tree with id: 0 has source attributes: [1, 2] has target attributes: [4], and predicts nominal attributes Tree with id: 1 has source attributes: [2, 3] has target attributes: [4], and predicts nominal attributes Tree with id: 2 has source attributes: [1, 3] has target attributes: [0, 2], and predicts numeric attributes Tree with id: 3 has source attributes: [4] has target attributes: [1, 3], and predicts numeric attributes Tree with id: 4 has source attributes: [1, 3, 4] has target attributes: [0, 2], and predicts numeric attributes Tree with id: 5 has source attributes: [2] has target attributes: [1, 3], and predicts numeric attributes And that concludes my quick tour of how to fit with MERCS. Prediction \u00b6 First, we generate a query. # Single target q_code = np . zeros ( clf . m_codes [ 0 ] . shape [ 0 ], dtype = int ) q_code [ - 1 :] = 1 print ( \"Query code is: {} \" . format ( q_code )) y_pred = clf . predict ( test , q_code = q_code ) y_pred [: 10 ] Query code is: [0 0 0 0 1] array([0., 0., 0., 1., 1., 1., 1., 0., 1., 0.]) clf . show_q_diagram () # Multi-target q_code = np . zeros ( clf . m_codes [ 0 ] . shape [ 0 ], dtype = int ) q_code [ - 2 :] = 1 print ( \"Query code is: {} \" . format ( q_code )) y_pred = clf . predict ( test , q_code = q_code ) y_pred [: 10 ] Query code is: [0 0 0 1 1] array([[ 0.06746825, 0. ], [-0.57445471, 1. ], [ 1.58481446, 0. ], [-0.57445471, 1. ], [-0.20623665, 0. ], [-0.20623665, 0. ], [-0.57445471, 1. ], [ 1.58481446, 0. ], [ 0.06746825, 1. ], [-0.20623665, 0. ]]) clf . show_q_diagram () # Missing attributes q_code = np . zeros ( clf . m_codes [ 0 ] . shape [ 0 ], dtype = int ) q_code [ - 1 :] = 1 q_code [: 1 ] = - 1 print ( \"Query code is: {} \" . format ( q_code )) y_pred = clf . predict ( test , q_code = q_code ) y_pred [: 10 ] Query code is: [-1 0 0 0 1] array([0., 0., 0., 0., 1., 1., 1., 1., 0., 0.]) clf . show_q_diagram ()","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#preliminaries","text":"","title":"Preliminaries"},{"location":"quickstart/#imports","text":"import mercs import numpy as np from mercs.tests import load_iris , default_dataset from mercs.core import Mercs import pandas as pd","title":"Imports"},{"location":"quickstart/#fit","text":"Here a small MERCS testdrive for what I suppose you'll need. First, let us generate a basic dataset. Some utility-functions are integrated in MERCS so that goes like this train , test = default_dataset ( n_features = 8 * 10 ** 0 ) df = pd . DataFrame ( train ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 -1.322151 -0.790957 0.516260 -0.484052 1.0 1 -1.813339 0.629799 0.971890 1.937495 1.0 2 -0.866028 -0.745873 0.348433 -1.046986 1.0 3 -0.431027 -0.196160 -2.822661 -1.445589 0.0 4 -2.512210 -3.314679 4.229655 0.147977 0.0 df . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 count 800.000000 800.000000 800.000000 800.000000 800.000000 mean -0.557043 -0.559566 0.006219 0.009929 0.495000 std 1.320172 1.395167 1.513970 1.541863 0.500288 min -4.843622 -4.530188 -4.497019 -4.653281 0.000000 25% -1.349024 -1.500278 -1.026567 -1.076323 0.000000 50% -0.705760 -0.698342 0.344050 0.040782 0.000000 75% 0.237204 0.271365 1.151239 1.071138 1.000000 max 3.371588 3.789293 4.229655 5.090814 1.000000 Now let's train a MERCS model. To know what options you have, come talk to me or dig in the code. For induction, nb_targets and nb_iterations matter most. Number of targets speaks for itself, number of iterations manages the amount of trees for each target . With n_jobs you can do multi-core learning (with joblib, really basic, but works fine on single machine), that makes stuff faster. fraction_missing sets the amount of attributes that is missing for a tree. However, this parameter only has an effect if you use the random selection algorithm. The alternative is the base algorithm, which selects targets, and uses all the rest as input. clf = Mercs ( max_depth = 4 , selection_algorithm = \"random\" , fraction_missing = 0.6 , nb_targets = 2 , nb_iterations = 2 , n_jobs = 8 , verbose = 1 , inference_algorithm = \"dask\" , max_steps = 8 , prediction_algorithm = \"it\" , ) You have to specify the nominal attributes yourself. This determines whether a regressor or a classifier is learned for that target. MERCS takes care of grouping targets such that no mixed sets are created. nominal_ids = { train . shape [ 1 ] - 1 } nominal_ids {4} clf . fit ( train , nominal_attributes = nominal_ids ) /cw/dtaijupiter/NoCsBack/dtai/elia/mercs/src/mercs/algo/induction.py:191: UserWarning: Training is being parallellized using Joblib. Number of jobs = 8 warnings.warn(msg) [Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=8)]: Done 6 out of 6 | elapsed: 2.3s finished /cw/dtaijupiter/NoCsBack/dtai/elia/miniconda/envs/rwrf/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples. 'precision', 'predicted', average, warn_for) So, now we have learned trees with two targets, but only a single target was nominal. If MERCS worked well, it should have learned single-target classifiers (for attribute 4) and multi-target regressors for all other target sets. clf . m_list [<mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf972151d0>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf97220650>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf97220710>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf9729cc90>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcf97204d10>, <mercs.composition.CanonicalModel.CanonicalModel at 0x7fcfd0465b50>] So, that looks good already. Let's examine up close. clf . m_codes array([[-1, 0, 0, -1, 1], [-1, -1, 0, 0, 1], [ 1, 0, 1, 0, -1], [-1, 1, -1, 1, 0], [ 1, 0, 1, 0, 0], [-1, 1, 0, 1, -1]]) That's the matrix that summarizes everything. This can be dense to parse, and there's alternatives to gain insights, for instance; for m_idx , m in enumerate ( clf . m_list ): msg = \"\"\" Tree with id: {} has source attributes: {} has target attributes: {} , and predicts {} attributes \"\"\" . format ( m_idx , m . desc_ids , m . targ_ids , m . out_kind ) print ( msg ) Tree with id: 0 has source attributes: [1, 2] has target attributes: [4], and predicts nominal attributes Tree with id: 1 has source attributes: [2, 3] has target attributes: [4], and predicts nominal attributes Tree with id: 2 has source attributes: [1, 3] has target attributes: [0, 2], and predicts numeric attributes Tree with id: 3 has source attributes: [4] has target attributes: [1, 3], and predicts numeric attributes Tree with id: 4 has source attributes: [1, 3, 4] has target attributes: [0, 2], and predicts numeric attributes Tree with id: 5 has source attributes: [2] has target attributes: [1, 3], and predicts numeric attributes And that concludes my quick tour of how to fit with MERCS.","title":"Fit"},{"location":"quickstart/#prediction","text":"First, we generate a query. # Single target q_code = np . zeros ( clf . m_codes [ 0 ] . shape [ 0 ], dtype = int ) q_code [ - 1 :] = 1 print ( \"Query code is: {} \" . format ( q_code )) y_pred = clf . predict ( test , q_code = q_code ) y_pred [: 10 ] Query code is: [0 0 0 0 1] array([0., 0., 0., 1., 1., 1., 1., 0., 1., 0.]) clf . show_q_diagram () # Multi-target q_code = np . zeros ( clf . m_codes [ 0 ] . shape [ 0 ], dtype = int ) q_code [ - 2 :] = 1 print ( \"Query code is: {} \" . format ( q_code )) y_pred = clf . predict ( test , q_code = q_code ) y_pred [: 10 ] Query code is: [0 0 0 1 1] array([[ 0.06746825, 0. ], [-0.57445471, 1. ], [ 1.58481446, 0. ], [-0.57445471, 1. ], [-0.20623665, 0. ], [-0.20623665, 0. ], [-0.57445471, 1. ], [ 1.58481446, 0. ], [ 0.06746825, 1. ], [-0.20623665, 0. ]]) clf . show_q_diagram () # Missing attributes q_code = np . zeros ( clf . m_codes [ 0 ] . shape [ 0 ], dtype = int ) q_code [ - 1 :] = 1 q_code [: 1 ] = - 1 print ( \"Query code is: {} \" . format ( q_code )) y_pred = clf . predict ( test , q_code = q_code ) y_pred [: 10 ] Query code is: [-1 0 0 0 1] array([0., 0., 0., 0., 1., 1., 1., 1., 0., 0.]) clf . show_q_diagram ()","title":"Prediction"}]}